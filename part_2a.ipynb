{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c6dda1-50d3-4bc2-b457-28bd68b5bb19",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/thesps/conifer/blob/master/conifer_v1.png?raw=true\" width=\"250\" alt=\"conifer\" />\n",
    "\n",
    "In this notebook we will learn how to build coprocessor images for BDTs with `conifer` using the model from `part_1`.\n",
    "\n",
    "We'll target Xilinx the Alveo U50 card.\n",
    "\n",
    "<img src=\"https://www.xilinx.com/content/dam/xilinx/imgs/kits/U50_Hero_1_Bracket.png\" width=250 alt=\"U50\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ee1e8-5e4d-490c-9189-92997b2fd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import conifer\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "\n",
    "# enable more output from conifer\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logger = logging.getLogger('conifer')\n",
    "logger.setLevel('DEBUG')\n",
    "\n",
    "# create a random seed at we use to make the results repeatable\n",
    "seed = int('fpga_tutorial'.encode('utf-8').hex(), 16) % 2**31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3cf2b-c9a2-46ed-b25d-188a49ef1f2b",
   "metadata": {},
   "source": [
    "## Register U50\n",
    "\n",
    "Firstly create and register the configuration of the Alveo U50 board that we will use. The part number and platform are board/system specific descriptions of the hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3a079-1501-4a3c-80f2-7e87a7d374b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "u50 = conifer.backends.boards.AlveoConfig.default_config()\n",
    "u50['xilinx_part'] = 'xcu50-fsvh2104-2-e'\n",
    "u50['platform'] = 'xilinx_u50_gen3x16_xdma_5_202210_1'\n",
    "u50['name'] = 'xilinx_u50_gen3x16_xdma_5_202210_1'\n",
    "u50 = conifer.backends.boards.AlveoConfig(u50)\n",
    "conifer.backends.boards.register_board_config(u50.name, u50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378d524-1893-48b7-89e7-c8e00904e091",
   "metadata": {},
   "source": [
    "# Accelerator Configuration\n",
    "Now that we've registered the board, we can use it as an accelerator target. To do that, we need to provide the `AcceleratorConfig` section that we previously left blank. This will add AXI interfaces to the data ports that we can then use to write and read data from the host. With this flow, since we're still selecting the `xilinxhls` backend, we will build a _static_ accelerator. This produces a bitfile that is specific to the single model we converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499f0d2-f864-47d4-8dd6-6dc0b02c4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = conifer.backends.xilinxhls.auto_config()\n",
    "\n",
    "# print the config\n",
    "print('Default Configuration\\n' + '-' * 50)\n",
    "print(json.dumps(cfg, indent=2))\n",
    "print('-' * 50)\n",
    "\n",
    "# modify the config\n",
    "accel_cfg = {'Board'         : u50.name,\n",
    "             'InterfaceType' : 'float',}\n",
    "cfg['AcceleratorConfig'] = accel_cfg\n",
    "cfg['OutputDir'] = 'prj_conifer_part_2'\n",
    "\n",
    "# print the config again\n",
    "print('Modified Configuration\\n' + '-' * 50)\n",
    "print(json.dumps(cfg, indent=2))\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98bd03d-492b-43a2-8a20-9113ec44424c",
   "metadata": {},
   "source": [
    "## Load model\n",
    "Load the model that we trained and saved in part 1, applying the new configuration described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672056e-8b98-45c6-80d2-fed95ac837b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conifer_model = conifer.model.load_model('prj_conifer_part_1/my_prj.json', new_config=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31755b51-d85e-4bfc-a6f4-055768572a47",
   "metadata": {},
   "source": [
    "## Build Static Accelerator\n",
    "\n",
    "Build the static accelerator binary.\n",
    "\n",
    "Firstly we run HLS C Synthesis. The result will be different to what we did in `part_1` since we've added the AXI interface that will introduce some overhead. At this point we export a `.xo` file that can be *linked* in the second step.\n",
    "\n",
    "Secondly we run Vitis (with `v++`) to build the bitfile which synthesizes all the necessary components, and runs place & route. Finally we obtain a `.xclbin` file that we can load to the FPGA.\n",
    "\n",
    "**Warning** the second part of this build will take around one hour to complete. When the message `Building Alveo bitfile ...` is displayed, check the file `vitis_build.log` for progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7f789-df20-4a4f-a01c-b328622e4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "conifer_model.write()\n",
    "conifer_model.build(synth=True, bitfile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06209d0c-e3e8-47d1-bb3f-5a5c068030ac",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "By default the Vivado project is removed after the `v++` step. If we instead build with `conifer_model.build(synth=True, bitfile=True, build_opts='--save-temps')`, we can view the floorplan. Here's an example of what that looks like, with the BDT model highlighted in green near the lower left. The orange section is the 'static region' that is reserved for the platform functions.\n",
    "\n",
    "<img src=\"images/conifer_hls_u50.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b781b80",
   "metadata": {},
   "source": [
    "## Runtime\n",
    "Load the binary onto the Alveo card using the conifer runtime. It is using Xilinx's `pynq` API that we already saw.\n",
    "We use the `AlveoDriver` from `conifer`, providing:\n",
    "- the `.xclbin` file that we built in the previous steps\n",
    "- the name of the 'ip' of the accelerator (by default conifer searches for IPs with `'conifer'` in the name)\n",
    "- the data batch size. This is used to allocate buffers for data transfer, and we can resize them later\n",
    "\n",
    "We also load the model JSON with the `conifer` Python backend to get a reference for the predictions.\n",
    "\n",
    "#### Detail\n",
    "\n",
    "In the driver, `conifer` loads the `.xclbin` file like `overlay = pynq.Overlay(bitfile)`\n",
    "\n",
    "Buffers are allocated with `pynq.allocate(shape, dtype)`. The shape has dimensions `(batch_size, n_features)`, and the `dtype` is the data type of the interface (not necessarily the same as the internal types).\n",
    "Predictions are always made on a full batch, so setting the batch size correctly is important for performance. In the FPGA, predictions over the batch are sequential, but we will transfer the full batch data in one go.\n",
    "\n",
    "The model binary itself stores some basic information about the model: the number of features and the number of classes that are used to define the buffer shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_u50 = conifer.backends.xilinxhls.runtime.AlveoDriver('prj_conifer_part_2/my_prj.xclbin', ip_name='my_prj_accelerator_1', batch_size=1)\n",
    "model_py = conifer.model.load_model('prj_conifer_part_2/my_prj.json', new_config={'backend':'py','output_dir':'dummy','project_name':'dummy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02edf3c0",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the test data from the model dataset. *Note* we need to make sure the data type matches the accelerator interface type (which is `float`), so we cast to `float32`. Otherwise, the prediction step will throw an error.\n",
    "We also set the batch size to resize the buffers, taking the first dimension of the data as the batch size: i.e. we will run prediction on the full data in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb143cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('moons_dataset/X_test.npy').astype('float32')\n",
    "model_u50._init_buffers(batch_size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718e7ba",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "Run inference on both the Alveo card and the Python model on CPU!\n",
    "\n",
    "#### Detail\n",
    "After checking data type and shape compatibility, the inference is executed on data `X` by the driver like:\n",
    "\n",
    "```\n",
    "Xbuf[:] = X                                               # copy the data into the buffer\n",
    "Xbuf.sync_to_device()                                     # transfer the buffer host->card\n",
    "ip.call(batch_size, null_buffer, null_buffer, Xbuf, ybuf) # call the accelerator\n",
    "while not ip.register_map.CTRL.AP_IDLE:                   # wait for the accelator to finish by polling the 'idle' status bit\n",
    "  pass\n",
    "ybuf.sync_from_device()                                   # transfer the buffer card->host\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f647e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_u50 = model_u50.decision_function(X_test)\n",
    "y_py = model_py.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05402bb-a295-41b4-96c5-5babefaf3adc",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Make some measurements of the time it takes to make predictions for the different methods. Use `time.perf_counter` or the `%%timeit` magic.\n",
    "\n",
    "Use the `xgboost` model rather than the `conifer` Python evaluation for the CPU reference. You can load it like:\n",
    "```\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model('prj_conifer_part_1/xgboost_model.json')\n",
    "```\n",
    "\n",
    "Try changing the batch size to see how that impacts the performance.\n",
    "\n",
    "Make a plot with batch size on the x axis and computation time on the y axis.\n",
    "\n",
    "To make more data for inference timing you can make some random numbers (for example with `np.random.rand`), generate a grid like in part 1 with more fine sampling, or use the `make_moons` method again with more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10aebe-0385-45cd-bbb5-be3a8a4c6733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
